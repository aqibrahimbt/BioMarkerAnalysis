---
title: "Pipeline to test all scripts"
output: html_notebook
---

Initialize libraries 
```{r}
packages <-c("ggplot2","jsonlite","stringr","Rtsne","digest","optparse","usdm", "caret", "e1071","httr","dplyr", "ranger", "randomForest","testthat","ROCR")
bio_packages <-c("DESeq2","genefilter")
packages = c(packages, bio_packages)
```


Generating packages
```{r}

suppressWarnings({
  suppressMessages({
    setwd("gToolbox/")
    lapply(packages, function(x) devtools::use_package(x))
    devtools::document()
    devtools::build()  
    devtools::test()
    })
})
install.packages("gToolbox_0.90.tar.gz",repos = NULL, type="source")
library(gToolbox)
```


Initialize global variables
```{r global}

# datasets = "GSE65367" # for classification
# datasets = "GSE62809"
# datasets = "GSE63046" # with covariates
# datasets = "GSE63501"
# datasets = "GSE39436"
# datasets = "GSE15229"
# datasets ="GSE31037" # psoriasis
datasets = "GSE37616" # renal
# datasets = "GSE46579" # alzheimer 
# datasets = "GSE19812" missing new sample group
log_level <<- "INFO"

formula = as.formula("~age+gender+condition")
output_dir = "./output"
working_dir = "./data/test_data/"
organism = "hsa"
dir.create(output_dir, showWarnings = FALSE)

```

Get metadata and transcript counts of dataset list
```{r io}

## Read meta data flat file (deprecated)
#meta_data_path = "./data/meta_data/meta_data_20180209.tsv"
#meta_data_table = read.metadata.table(meta_data_path)
#meta_data = meta_data_table
#meta_data = meta_data_table[meta_data_table$dataset==datasets,]
## Read meta data from api DB
meta_data = get_metadata(datasets)

dataset_data = get.data(datasets,meta_data, working_dir, file = "featureCounts.tsv", formula=formula)

count_matrix = dataset_data$count.matrix
feature_length= dataset_data$feature.length
conditions = dataset_data$conditions

```

Write list of packages and versions to json
```{r}
file_name = "./output/packages.json"
write_package_list(file_name, datasets)
```


Calcualate RPM
```{r normalization}
dataset_data_rpm = normalize_data(dataset_data,"rpm")
```

Calculate differential feature expression
```{r}

dds <- de_result(dataset_data)
sample_group = dataset_data$sample_group
de_comparisons(dds, datasets, output_dir,sample_group,dataset_data$formula, dataset_data$summary$mapping$genome, overwrite=F, shrink=T)

## Count data transformations 
vst <- DESeq2::varianceStabilizingTransformation(dds)

dds_heatmap_gene_counts(dds, 30, datasets,output_dir)
dds_heatmap_rld_vst(vst, 30, datasets,output_dir,"vst")

dds_heatmap_distance(vst, datasets,output_dir, "vst")
dds_PCA(vst, datasets, output_dir,sample_group,"vst",T)
dds_TSNE(vst, datasets, output_dir,sample_group,5, "vst",T)

```


Testing Classification
```{r}
library(doParallel)

cl <- makeCluster(2)
registerDoParallel(cl)

#####
# Select condition of interrest (count data, labels)
comparisons = get_comparisons(dataset_data$conditions, dataset_data$sample_group)
dummy_comparision = comparisons$conditions[1,]
samples_con1 = comparisons$samples[[as.character(dummy_comparision[1,1])]]
samples_con2 = comparisons$samples[[as.character(dummy_comparision[1,2])]]

norm_data = normalize_data(dataset_data, "deseq")
sub_samples = c(samples_con1,samples_con2)
countdata = as.data.frame(t(norm_data[,sub_samples]))
conditions = dataset_data$conditions[rownames(countdata),]

#####  
#recursive feature elimination
profiles = recursive_feature_elimination(countdata, conditions, repeats = 2, number = 5)

#####
# aggregate accuracy data for results
aggregated_data = aggregate_profiles(profiles)
profile_size = length(profiles)

##### 
#get dataset with Optimized variables and save logscale
optimized_data <-as.data.frame(countdata[,profiles[[profile_size]]$optVariables])
cv_feature_elimination_log = feature_logscale(optimized_data, aggregated_data, profiles[[profile_size]])

write_to_json(datasets, cv_feature_elimination_log, "Cross-validated prediction error of random forest models trained by incrementally adding features according to their gini ranking.", output_dir, "feature_selection")

#####
# calculate final model
final_rf_model = get_final_model(optimized_data,conditions)

#####
# cross validation
cv_final_model = cross_validation(5,5,optimized_data,conditions)

#####
# cross validation error
cv_err = cv_error(cv_final_model)
write_to_json(datasets, cv_err, "Cross-validated error plot per sample", output_dir, "cv_error")

#####
# cross validation feature importance
cv_fi = cv_feature_importance(cv_final_model,output_dir)
write_to_json(datasets, cv_fi, "Quantiles of cross-validation feature importance", output_dir, "cv_feature_importance")

#####
# roc curve and auc
roc_values = roc(final_rf_model)
write_to_json(datasets, roc_values , "ROC curve and AUC value", output_dir, "roc")

#####
# out of back error
oob_values = oob(final_rf_model)
write_to_json(datasets, oob_values , "out-of-bag error(OOB) obtained for each independent tree that forms the forest", output_dir, "oob")

```
Testing RCE-SVM
```{r}

no_clusters = 10
required_no_clusters = 5
cond <- as.data.frame(conditions)
input <- as.data.frame(cbind(conditions, countdata))
colnames(input)[1] <- 'Conditions'
optimized_data <- suppressWarnings(rce_svm(input, no_clusters, required_no_clusters))

final_rf_model <- get_final_model(optimized_data,conditions)
# cross validation
cv_final_model = cross_validation(5,5,optimized_data,conditions)

#####
# cross validation error
cv_err = cv_error(cv_final_model)
write_to_json(datasets, cv_err, "Cross-validated error plot per sample", output_dir, "cv_error")

#####
# cross validation feature importance
cv_fi = cv_feature_importance(cv_final_model,output_dir)
write_to_json(datasets, cv_fi, "Quantiles of cross-validation feature importance", output_dir, "cv_feature_importance")

#####
# roc curve and auc
roc_values = roc(final_rf_model)
write_to_json(datasets, roc_values , "ROC curve and AUC value", output_dir, "roc")

#####
# out of back error
oob_values = oob(final_rf_model)
write_to_json(datasets, oob_values , "out-of-bag error(OOB) obtained for each independent tree that forms the forest", output_dir, "oob")

```

