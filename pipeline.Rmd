---
title: "Pipeline to test all scripts"
output: html_notebook
---

Initialize libraries 
```{r}
packages <-c("ggplot2","jsonlite","stringr","Rtsne","digest","optparse","usdm", "caret", "e1071","httr","dplyr", "ranger", "randomForest","testthat")
bio_packages <-c("DESeq2","genefilter")
packages = c(packages, bio_packages)
```


Generating packages
```{r}

suppressWarnings({
  suppressMessages({
    setwd("gToolbox/")
    lapply(packages, function(x) devtools::use_package(x))
    devtools::document()
    devtools::build()  
    devtools::test()
    })
})
#install.packages("gToolbox_0.90.tar.gz",repos = NULL, type="source")
library(gToolbox)
```


Initialize global variables
```{r global}

# datasets = "GSE65367" # for classification
# datasets = "GSE62809"
datasets = "GSE63046" # with covariates
# datasets = "GSE63501"
# datasets = "GSE39436"
# datasets = "GSE15229"
# datasets ="GSE31037" # psoriasis
# datasets = "GSE37616" # renal
# datasets = "GSE46579"
# datasets = "GSE19812" missing new sample group
log_level <<- "INFO"

formula = as.formula("~age+gender+condition")
output_dir = "./output"
working_dir = "./data/test_data/"
organism = "hsa"
dir.create(output_dir, showWarnings = FALSE)

```

Get metadata and transcript counts of dataset list
```{r io}

## Read meta data flat file (deprecated)
meta_data_path = "./data/meta_data/meta_data_20180209.tsv"
meta_data_table = read.metadata.table(meta_data_path)
#meta_data = meta_data_table
## Read meta data from api DB
meta_data = get_metadata(datasets)

meta_data = meta_data_table[meta_data_table$dataset==datasets,]
#meta_data$new.sample.group = "tissue phenotype+disease details"

dataset_data = get.data(datasets,meta_data, working_dir, file = "featureCounts.tsv", formula=formula)

count_matrix = dataset_data$count.matrix
feature_length= dataset_data$feature.length
conditions = dataset_data$conditions

```

Write list of packages and versions to json
```{r}
file_name = "./output/packages.json"
write_package_list(file_name, datasets)
```


Calcualate RPM
```{r normalization}
dataset_data_rpm = normalize_data(dataset_data,"rpm")
```

Calculate differential feature expression
```{r}

dds <- de_result(dataset_data)
sample_group = dataset_data$sample_group
de_comparisons(dds, datasets, output_dir,sample_group,dataset_data$formula, overwrite=F, shrink=T)

## Count data transformations 
vst <- DESeq2::varianceStabilizingTransformation(dds)

dds_heatmap_gene_counts(dds, 30, datasets,output_dir)
dds_heatmap_rld_vst(vst, 30, datasets,output_dir,"vst")

dds_heatmap_distance(vst, datasets,output_dir, "vst")
dds_PCA(vst, datasets, output_dir,sample_group,"vst",T)
dds_TSNE(vst, datasets, output_dir,sample_group,5, "vst",T)

```


Testing Classification
```{r}
library(randomForest)
library(doParallel)
library(ranger)
cl <- makeCluster(2)
registerDoParallel(cl)


# Select condition of interrest (count data, labels)
comparisons = get_comparisons(dataset_data$conditions, dataset_data$sample_group)
dummy_comparision = comparisons$conditions[1,]
samples_con1 = comparisons$samples[[as.character(dummy_comparision[1,1])]]
samples_con2 = comparisons$samples[[as.character(dummy_comparision[1,2])]]

norm_data = normalize_data(dataset_data, "deseq")
sub_samples = c(samples_con1,samples_con2)
countdata = as.data.frame(t(norm_data[,sub_samples]))
conditions = dataset_data$conditions[rownames(countdata),]
  
#Step 2: Recursive feature elimination
  
profiles = recursive_feature_elimination(countdata, conditions, repeats = 2, number = 5)
  
#####################################################################################################################
#Step 3: Aggregate data for results
aggregated_data = aggregate_profiles(profiles)
  
profile_size = length(profiles)
  
#####################################################################################################################
#Step 4: Get dataset with Optimized variables and save logscale
optimized_data <-as.data.frame(countdata[,profiles[[profile_size]]$optVariables])
feature_logscale(optimized_data, aggregated_data, profiles[[profile_size]], output_dir)
  
#####################################################################################################################
#Step 5: Run Random Forest Classification
mtry = c(1:5)
rf.model <- train_model(optimized_data, conditions, "rf", mtry  ,repeats=2, number=5, balanced = F)


```

```{r}
library(caret)
library(mlbench)
library(Hmisc)
library(randomForest)

data(PimaIndiansDiabetes)
data = PimaIndiansDiabetes

set.seed(1234)
ind <- sample(2, nrow(data), replace=TRUE, prob=c(0.7, 0.3))
trainData <- data[ind==1,]
validationData <- data[ind==2,]
trainData <- trainData[complete.cases(trainData),]
validationData <- validationData[complete.cases(validationData),]

trainData[,1:8]
trainData[,9]
```
